{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72080095",
   "metadata": {},
   "source": [
    "# Implementation of Skip-Gram model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea64dcc1",
   "metadata": {},
   "source": [
    "While a bag-of-words model predicts a word given the neighboring context, a skip-gram model predicts the context (or neighbors) of a word, given the word itself. The model is trained on skip-grams, which are n-grams that allow tokens to be skipped (see the diagram below for an example). The context of a word can be represented through a set of skip-gram pairs of `(target_word, context_word)` where `context_word` appears in the neighboring context of `target_word`.\n",
    "\n",
    "The training objective of the skip-gram model is to maximize the probability of predicting context words given the target word. For a sequence of words *w<sub>1</sub>, w<sub>2</sub>, ... w<sub>T</sub>*, the objective can be written as the average log probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8af6219",
   "metadata": {},
   "source": [
    "![word2vec_skipgram_objective](https://tensorflow.org/text/tutorials/images/word2vec_skipgram_objective.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac3b2fc",
   "metadata": {},
   "source": [
    "where `c` is the size of the training context. The basic skip-gram formulation defines this probability using the softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ea7d17",
   "metadata": {},
   "source": [
    "![word2vec_full_softmax](https://tensorflow.org/text/tutorials/images/word2vec_full_softmax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0f4e27",
   "metadata": {},
   "source": [
    "where *v* and *v<sup>'<sup>* are target and context vector representations of words and *W* is vocabulary size.\n",
    "\n",
    "Computing the denominator of this formulation involves performing a full softmax over the entire vocabulary words, which are often large (10<sup>5</sup>-10<sup>7</sup>) terms.\n",
    "\n",
    "The [noise contrastive estimation](https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss) (NCE) loss function is an efficient approximation for a full softmax. With an objective to learn word embeddings instead of modeling the word distribution, the NCE loss can be [simplified](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) to use negative sampling.\n",
    "\n",
    "The simplified negative sampling objective for a target word is to distinguish  the context word from `num_ns` negative samples drawn from noise distribution *P<sub>n</sub>(w)* of words. More precisely, an efficient approximation of full softmax over the vocabulary is, for a skip-gram pair, to pose the loss for a target word as a classification problem between the context word and `num_ns` negative samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2bd653",
   "metadata": {},
   "source": [
    "**Objective of the Skip-gram Model with Negative Sampling**\n",
    "\n",
    "For a pair consisting of a **target word** $ w_t $ and a **context word** $ w_c $,  \n",
    "and $ k $ negative samples drawn from the noise distribution $ P_n(w) $,  \n",
    "the loss function (to be minimized) is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = - \\left[\n",
    "\\log \\sigma \\left( v_{w_c}^\\top v_{w_t} \\right)\n",
    "+ \\sum_{i=1}^{k} \n",
    "\\mathbb{E}_{w_i \\sim P_n(w)} \n",
    "\\left[ \n",
    "\\log \\sigma \\left( - v_{w_i}^\\top v_{w_t} \\right)\n",
    "\\right]\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Where:\n",
    "\n",
    "- $ v_{w_t}$: vector of the **target word**  \n",
    "- $ v_{w_c} $ vector of the **context word**  \n",
    "- $ v_{w_i} $: vector of a **negative word** sampled from $ P_n(w) $ \n",
    "- $ \\sigma(x) = \\frac{1}{1 + e^{-x}} $: the **sigmoid function**  \n",
    "- $ k = \\text{num\\_ns} $: number of negative samples  \n",
    "- $ P_n(w) $: **noise distribution**\n",
    "\n",
    "---\n",
    "\n",
    "Intuitive interpretation:\n",
    "\n",
    "- The first term $ \\log \\sigma(v_{w_c}^\\top v_{w_t}) $ **maximizes the similarity** between words that appear in the **true context**.  \n",
    "- The second term $ \\sum_{i=1}^{k} \\log \\sigma(-v_{w_i}^\\top v_{w_t}) $ **minimizes the similarity** with **negative (noise) words**.  \n",
    "- Therefore, the model learns to **distinguish real contexts from false ones**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aae7a2",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27918c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lioula/User/PFE/Make.Org/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Now we have our own vocabulary words we will now implement a skip-gram model using TensorFlow and Keras.\n",
    "# project the words into a vector space where similar words are closer together using K-means clustering.\n",
    "\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import spacy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e0f6fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcc8aa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61d2d56",
   "metadata": {},
   "source": [
    "## Prepare training data for word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "868f53f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Proposition  \\\n",
      "0  favoriser le reboisement urbain afin de baisse...   \n",
      "1  stopper lâ€™Ã©rosion des haies (-23 500km par an)...   \n",
      "2  stopper l&apos;importation de produits agricol...   \n",
      "3  rÃ©intÃ©grer les arbres et les haies dans lâ€™agri...   \n",
      "4  empÃªcher l&apos;importation de produits agrico...   \n",
      "5  replanter des haies pour Ã©viter l&apos;Ã©rosion...   \n",
      "6  punir sÃ©vÃ¨rement ceux qui dÃ©versent des dÃ©chet...   \n",
      "7  replanter des haies autour des champs afin de ...   \n",
      "8  prÃ©server, restaurer, maintenir en bon Ã©tat Ã©c...   \n",
      "9  soutenir une agriculture protectrice du vivant...   \n",
      "\n",
      "                                       Tokens_lemmas  \n",
      "0  [favoriser, reboisement, urbain, baisser, temp...  \n",
      "1  [stopper, Ã©rosion, haie, kilomÃ¨tre, an, puisse...  \n",
      "2  [stopper, produit, agricole, traiter, produit,...  \n",
      "3  [rÃ©intÃ©grer, arbre, haie, agriculture, produir...  \n",
      "4  [empÃªcher, produit, agricole, traiter, produit...  \n",
      "5  [replanter, haie, Ã©viter, sol, permettre, faun...  \n",
      "6  [punir, sÃ©vÃ¨rement, dÃ©verser, dÃ©chet, nature, ...  \n",
      "7  [replanter, haie, autour, champ, prÃ©server, bi...  \n",
      "8  [prÃ©server, restaurer, maintenir, bon, Ã©tat, Ã©...  \n",
      "9  [soutenir, agriculture, protecteur, vivant, pe...  \n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# CONFIGURATION\n",
    "# ================================\n",
    "INPUT = \"makeorg_biodiversite.xlsx\"\n",
    "OUTPUT = \"output.xlsx\"\n",
    "TEXT_COL = \"Proposition\"\n",
    "NEW_COL = \"Tokens_lemmas\"\n",
    "MODEL = \"fr_core_news_sm\"\n",
    "\n",
    "# ================================\n",
    "# LOAD MODEL (auto-download if missing)\n",
    "# ================================\n",
    "try:\n",
    "    nlp = spacy.load(MODEL)\n",
    "except OSError:\n",
    "    print(f\"Downloading {MODEL} ...\")\n",
    "    from spacy.cli import download\n",
    "    download(MODEL)\n",
    "    nlp = spacy.load(MODEL)\n",
    "\n",
    "# ================================\n",
    "# READ DATA\n",
    "# ================================\n",
    "df = pd.read_excel(INPUT)\n",
    "\n",
    "if TEXT_COL not in df.columns:\n",
    "    raise ValueError(f\"Column '{TEXT_COL}' not found. Available: {list(df.columns)}\")\n",
    "\n",
    "# Optional: remove leading â€œIl faut â€¦â€\n",
    "df[TEXT_COL] = df[TEXT_COL].str.replace(r'^\\s*Il faut\\s+', '', case=False, regex=True)\n",
    "\n",
    "# ================================\n",
    "# DEFINE CLEANING FUNCTION\n",
    "# ================================\n",
    "def doc_to_lemmas(doc):\n",
    "    \"\"\"\n",
    "    Clean and lemmatize text automatically using spaCy:\n",
    "    - Removes punctuation, numbers, and stopwords (articles, pronouns, etc.)\n",
    "    - Keeps meaningful lemmas (nouns, verbs, adjectives, adverbs)\n",
    "    \"\"\"\n",
    "    lemmas = []\n",
    "    for tok in doc:\n",
    "        if tok.is_punct or tok.is_space:\n",
    "            continue\n",
    "        if tok.is_stop:  # automatic stopword detection \n",
    "            continue\n",
    "        if tok.like_num or any(ch.isdigit() for ch in tok.text):\n",
    "            continue\n",
    "        if tok.is_alpha or any(ch.isdigit() for ch in tok.text):\n",
    "            lemma = tok.lemma_.lower()\n",
    "            lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "# ================================\n",
    "# PROCESS TEXTS\n",
    "# ================================\n",
    "texts = df[TEXT_COL].astype(str).fillna(\"\").tolist()\n",
    "docs = nlp.pipe(texts, disable=[\"parser\", \"ner\"])  # faster\n",
    "\n",
    "df[NEW_COL] = [doc_to_lemmas(doc) for doc in docs]\n",
    "\n",
    "# ================================\n",
    "# SAVE RESULT\n",
    "# ================================\n",
    "df.to_excel(OUTPUT, index=False)\n",
    "\n",
    "# show first few samples\n",
    "print(df[[TEXT_COL, NEW_COL]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29e530a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“˜ Vocabulary size: 6033 unique lemmas\n",
      "\n",
      " Top 20 most frequent words:\n",
      "biodiversitÃ©         690\n",
      "interdire            454\n",
      "faire                328\n",
      "chasse               315\n",
      "arrÃªter              311\n",
      "nature               311\n",
      "animal               303\n",
      "zone                 274\n",
      "crÃ©er                266\n",
      "protÃ©ger             258\n",
      "espÃ¨ce               233\n",
      "limiter              230\n",
      "ville                218\n",
      "haie                 210\n",
      "arbre                208\n",
      "naturel              208\n",
      "produit              204\n",
      "favoriser            202\n",
      "mettre               201\n",
      "interdir             198\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "# Flatten all token lists into one big list\n",
    "all_tokens = list(chain.from_iterable(df[\"Tokens_lemmas\"]))\n",
    "\n",
    "# Count frequency of each word\n",
    "vocab_counter = Counter(all_tokens)\n",
    "\n",
    "# Vocabulary size = number of unique words\n",
    "vocab_size = len(vocab_counter)\n",
    "print(f\"ğŸ“˜ Vocabulary size: {vocab_size} unique lemmas\")\n",
    "\n",
    "# Show 20 most frequent words\n",
    "print(\"\\n Top 20 most frequent words:\")\n",
    "for word, freq in vocab_counter.most_common(20):\n",
    "    print(f\"{word:20s} {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c8e9a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    df[\"Tokens_lemmas\"].apply(lambda tokens: \" \".join(tokens)).values\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cde9880",
   "metadata": {},
   "source": [
    "The TextVectorization layer in TensorFlow/Keras converts raw text into numerical tensors that a model can understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b8c7515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, create a custom standardization function to lowercase the text and\n",
    "# remove punctuation.\n",
    "def custom_standardization(input_data):\n",
    "  return tf.strings.lower(input_data) # already cleaned with spaCy\n",
    "\n",
    "\n",
    "# Define the vocabulary size and the number of words in a sequence.\n",
    "vocab_size = 6033 # from previous cell\n",
    "sequence_length = int(df[\"Tokens_lemmas\"].apply(len).quantile(0.90))\n",
    "\n",
    "# Use the `TextVectorization` layer to normalize, split, and map strings to\n",
    "# integers. Set the `output_sequence_length` length to pad all samples to the\n",
    "# same length.\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f85b607",
   "metadata": {},
   "source": [
    "Call `TextVectorization.adapt` on the text dataset to create vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de92d745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 01:08:40.730206: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "vectorize_layer.adapt(text_ds.batch(1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0aef11",
   "metadata": {},
   "source": [
    "Once the state of the layer has been adapted to represent the text corpus, the vocabulary can be accessed with `TextVectorization.get_vocabulary`. This function returns a list of all vocabulary tokens sorted (descending) by their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07480dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', np.str_('biodiversitÃ©'), np.str_('interdire'), np.str_('faire'), np.str_('chasse'), np.str_('nature'), np.str_('arrÃªter'), np.str_('animal'), np.str_('zone'), np.str_('crÃ©er'), np.str_('protÃ©ger'), np.str_('espÃ¨ce'), np.str_('limiter'), np.str_('ville'), np.str_('haie'), np.str_('naturel'), np.str_('arbre'), np.str_('produit'), np.str_('favoriser')]\n"
     ]
    }
   ],
   "source": [
    "# Save the created vocabulary for reference.\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e011e4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data in text_ds.\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ba5d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for `vocab_size` tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=seed,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a712aa8",
   "metadata": {},
   "source": [
    "### Obtain sequences from the dataset\n",
    "\n",
    "You now have a `tf.data.Dataset` of integer encoded sentences. To prepare the dataset for training a word2vec model, flatten the dataset into a list of sentence vector sequences. This step is required as you would iterate over each sentence in the dataset to produce positive and negative examples.\n",
    "\n",
    "Note: Since the `generate_training_data()` defined earlier uses non-TensorFlow Python/NumPy functions, you could also use a `tf.py_function` or `tf.numpy_function` with `tf.data.Dataset.map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9bbb62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 01:08:51.350435: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c98432f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  19 1643  125  564  650   14 1331    0    0    0    0] => [np.str_('favoriser'), np.str_('reboisement'), np.str_('urbain'), np.str_('baisser'), np.str_('tempÃ©rature'), np.str_('ville'), np.str_('capter'), '', '', '', '']\n",
      "[ 129 1848   15  836  106 3955 2066  610   64   82    0] => [np.str_('stopper'), np.str_('Ã©rosion'), np.str_('haie'), np.str_('kilomÃ¨tre'), np.str_('an'), np.str_('puisser'), np.str_('levier'), np.str_('transition'), np.str_('Ã©cologique'), np.str_('territoire'), '']\n",
      "[129  18  43 877  18   3  76   0   0   0   0] => [np.str_('stopper'), np.str_('produit'), np.str_('agricole'), np.str_('traiter'), np.str_('produit'), np.str_('interdire'), np.str_('france'), '', '', '', '']\n",
      "[1625   17   15  160  211   11    2  669 1343    0    0] => [np.str_('rÃ©intÃ©grer'), np.str_('arbre'), np.str_('haie'), np.str_('agriculture'), np.str_('produire'), np.str_('protÃ©ger'), np.str_('biodiversitÃ©'), np.str_('grÃ¢ce'), np.str_('agroforesterie'), '', '']\n",
      "[285  18  43 877  18   3  76   0   0   0   0] => [np.str_('empÃªcher'), np.str_('produit'), np.str_('agricole'), np.str_('traiter'), np.str_('produit'), np.str_('interdire'), np.str_('france'), '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences[:5]:\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fb5faf",
   "metadata": {},
   "source": [
    "## Generate training examples from sequences\n",
    "\n",
    "`sequences` is now a list of int encoded sentences. Just call the `generate_training_data` function defined earlier to generate training examples for the word2vec model. To recap, the function iterates over each word from each sequence to collect positive and negative context words. Length of target, contexts and labels should be the same, representing the total number of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96dee19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5554/5554 [00:02<00:00, 2350.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "targets.shape: (26599,)\n",
      "contexts.shape: (26599, 5)\n",
      "labels.shape: (26599, 5)\n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=2,  #############\n",
    "    num_ns=4,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=SEED)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0290cdcb",
   "metadata": {},
   "source": [
    "## Configure the dataset for performance\n",
    "\n",
    "To perform efficient batching for the potentially large number of training examples, use the `tf.data.Dataset` API. After this step, you would have a `tf.data.Dataset` object of `(target_word, context_word), (label)` elements to train your word2vec model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bb0342f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_BatchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a68e1e",
   "metadata": {},
   "source": [
    "Apply `Dataset.cache` and `Dataset.prefetch` to improve performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "904017dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b46a31",
   "metadata": {},
   "source": [
    "## Model and training\n",
    "\n",
    "Use the [Keras Subclassing API](https://www.tensorflow.org/guide/keras/custom_layers_and_models) to define your word2vec model with the following layers:\n",
    "\n",
    "* `target_embedding`: A `tf.keras.layers.Embedding` layer, which looks up the embedding of a word when it appears as a target word. The number of parameters in this layer are `(vocab_size * embedding_dim)`.\n",
    "* `context_embedding`: Another `tf.keras.layers.Embedding` layer, which looks up the embedding of a word when it appears as a context word. The number of parameters in this layer are the same as those in `target_embedding`, i.e. `(vocab_size * embedding_dim)`.\n",
    "* `dots`: A `tf.keras.layers.Dot` layer that computes the dot product of target and context embeddings from a training pair.\n",
    "* `flatten`: A `tf.keras.layers.Flatten` layer to flatten the results of `dots` layer into logits.\n",
    "\n",
    "With the subclassed model, you can define the `call()` function that accepts `(target, context)` pairs which can then be passed into their corresponding embedding layer. Reshape the `context_embedding` to perform a dot product with `target_embedding` and return the flattened result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4f70b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      name=\"w2v_embedding\")\n",
    "    self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                       embedding_dim)\n",
    "\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "    # context: (batch, context)\n",
    "    if len(target.shape) == 2:\n",
    "      target = tf.squeeze(target, axis=1)\n",
    "    # target: (batch,)\n",
    "    word_emb = self.target_embedding(target)\n",
    "    # word_emb: (batch, embed)\n",
    "    context_emb = self.context_embedding(context)\n",
    "    # context_emb: (batch, context, embed)\n",
    "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "    # dots: (batch, context)\n",
    "    return dots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d1229b",
   "metadata": {},
   "source": [
    "## Define loss function and compile model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099a1326",
   "metadata": {},
   "source": [
    "For simplicity, you can use `tf.keras.losses.CategoricalCrossEntropy` as an alternative to the negative sampling loss. If you would like to write your own custom loss function, you can also do so as follows:\n",
    "\n",
    "``` python\n",
    "def custom_loss(x_logit, y_true):\n",
    "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)\n",
    "```\n",
    "\n",
    "It's time to build your model! Instantiate your word2vec class with an embedding dimension of 128 (you could experiment with different values). Compile the model with the `tf.keras.optimizers.Adam` optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57cfd411",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8fa21d",
   "metadata": {},
   "source": [
    "Also define a callback to log training statistics for TensorBoard:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b478fa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb8fa1b",
   "metadata": {},
   "source": [
    "Train the model on the `dataset` for some number of epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f19a752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.2026 - loss: 1.6093\n",
      "Epoch 2/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7309 - loss: 1.5942\n",
      "Epoch 3/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9012 - loss: 1.5792\n",
      "Epoch 4/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9501 - loss: 1.5545\n",
      "Epoch 5/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9566 - loss: 1.5100\n",
      "Epoch 6/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9501 - loss: 1.4395\n",
      "Epoch 7/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9408 - loss: 1.3529\n",
      "Epoch 8/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9343 - loss: 1.2623\n",
      "Epoch 9/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9360 - loss: 1.1705\n",
      "Epoch 10/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9432 - loss: 1.0766\n",
      "Epoch 11/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9522 - loss: 0.9808\n",
      "Epoch 12/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9596 - loss: 0.8852\n",
      "Epoch 13/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9671 - loss: 0.7923\n",
      "Epoch 14/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9717 - loss: 0.7043\n",
      "Epoch 15/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9770 - loss: 0.6229\n",
      "Epoch 16/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9809 - loss: 0.5492\n",
      "Epoch 17/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9830 - loss: 0.4834\n",
      "Epoch 18/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9855 - loss: 0.4254\n",
      "Epoch 19/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9879 - loss: 0.3747\n",
      "Epoch 20/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9901 - loss: 0.3308\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x17904c0d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7873b46d",
   "metadata": {},
   "source": [
    "TensorBoard now shows the word2vec model's accuracy and loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c5270e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 92676), started 0:14:18 ago. (Use '!kill 92676' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c8340d304b7e8888\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c8340d304b7e8888\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#docs_infra: no_execute\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5390ed71",
   "metadata": {},
   "source": [
    "## Embedding lookup and analysis\n",
    "\n",
    "Obtain the weights from the model using `Model.get_layer` and `Layer.get_weights`. The `TextVectorization.get_vocabulary` function provides the vocabulary to build a metadata file with one token per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e86ac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b57e3eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\") \n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6849e269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de mots dans le vocabulaire : 6032\n",
      "Dimension des embeddings : 128\n",
      "RÃ©duction des dimensions avec t-SNE (cela peut prendre 1-2 min)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lioula/User/PFE/Make.Org/.venv/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning:\n",
      "\n",
      "'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 50\u001b[0m\n\u001b[1;32m     41\u001b[0m fig \u001b[38;5;241m=\u001b[39m px\u001b[38;5;241m.\u001b[39mscatter(\n\u001b[1;32m     42\u001b[0m     emb_df,\n\u001b[1;32m     43\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m     width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m900\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m700\u001b[39m\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     49\u001b[0m fig\u001b[38;5;241m.\u001b[39mupdate_traces(marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, opacity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)))\n\u001b[0;32m---> 50\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/User/PFE/Make.Org/.venv/lib/python3.9/site-packages/plotly/basedatatypes.py:3420\u001b[0m, in \u001b[0;36mBaseFigure.show\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3387\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3388\u001b[0m \u001b[38;5;124;03mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[1;32m   3389\u001b[0m \u001b[38;5;124;03mspecified by the renderer argument\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3416\u001b[0m \u001b[38;5;124;03mNone\u001b[39;00m\n\u001b[1;32m   3417\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3418\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpio\u001b[39;00m\n\u001b[0;32m-> 3420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/User/PFE/Make.Org/.venv/lib/python3.9/site-packages/plotly/io/_renderers.py:415\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    412\u001b[0m     )\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nbformat \u001b[38;5;129;01mor\u001b[39;00m Version(nbformat\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    416\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m     )\n\u001b[1;32m    419\u001b[0m display_jupyter_version_warnings()\n\u001b[1;32m    421\u001b[0m ipython_display\u001b[38;5;241m.\u001b[39mdisplay(bundle, raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# Visualisation des embeddings avec K-means + t-SNE\n",
    "# =====================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "\n",
    "# 1ï¸âƒ£ Extraire les poids et le vocabulaire\n",
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "\n",
    "# Supprimer le token de padding (index 0)\n",
    "weights = weights[1:]\n",
    "vocab = vocab[1:]\n",
    "\n",
    "print(f\"Nombre de mots dans le vocabulaire : {len(vocab)}\")\n",
    "print(f\"Dimension des embeddings : {weights.shape[1]}\")\n",
    "\n",
    "# 2ï¸âƒ£ Clustering K-means\n",
    "n_clusters = 10  # tu peux ajuster (5, 10, 20 selon ton vocabulaire)\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(weights)\n",
    "\n",
    "# 3ï¸âƒ£ RÃ©duction de dimension (t-SNE)\n",
    "print(\"RÃ©duction des dimensions avec t-SNE (cela peut prendre 1-2 min)...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "reduced = tsne.fit_transform(weights)\n",
    "\n",
    "# 4ï¸âƒ£ Construire un DataFrame pour la visualisation\n",
    "emb_df = pd.DataFrame({\n",
    "    \"x\": reduced[:, 0],\n",
    "    \"y\": reduced[:, 1],\n",
    "    \"word\": vocab,\n",
    "    \"cluster\": clusters\n",
    "})\n",
    "\n",
    "# 5ï¸âƒ£ Visualisation interactive avec Plotly\n",
    "fig = px.scatter(\n",
    "    emb_df,\n",
    "    x=\"x\", y=\"y\",\n",
    "    color=\"cluster\",\n",
    "    hover_name=\"word\",\n",
    "    title=\"Visualisation des embeddings Word2Vec (t-SNE + K-means)\",\n",
    "    width=900, height=700\n",
    ")\n",
    "fig.update_traces(marker=dict(size=8, opacity=0.8, line=dict(width=0)))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c95423b",
   "metadata": {},
   "source": [
    "Download the `vectors.tsv` and `metadata.tsv` to analyze the obtained embeddings in the [Embedding Projector](https://projector.tensorflow.org/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edef6237",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import files\n",
    "  files.download('vectors.tsv')\n",
    "  files.download('metadata.tsv')\n",
    "except Exception:\n",
    "  pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
